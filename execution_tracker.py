import pdb
import sys
import copy
import linecache

from random import random


class ExecutionTracker(pdb.Pdb):
    def __init__(self, program_inputs, finetuned_model=None, *args, **kwargs):
        super(ExecutionTracker, self).__init__(*args, **kwargs)
        self.program_inputs = program_inputs

        # this is set when the user wants to do all the calls that call an llm using a single finetuned model (inference mode)
        # if this is set to None, the program is just executed and traced.
        self.finetuned_model = finetuned_model
        # generated variables holds the values of variables that were generated by the finetuned model if it was set
        self.generated_variables = {}

        self.trace_string = ""

        self.prev_local_dict = None
        self.prev_line = None

        self.tracing_enabled = False
        self.first = True
        
    
    def first_write(self):
        self.trace_string += f"inputs:\n"
        for k, v in self.program_inputs.items():
            self.trace_string += f"{k} = {v}\n\n"

        self.trace_string += f"Execution:\n\n"
    
    def is_llm_call(self, line):
        # TODO: simple check now, we want to instead check whether this is a dspy module call
        return "llm" in line

    def call_finetuned_model(self, line, current_local_dict):
        # first, parse the input names from the line
        import re
        inputs = re.findall(r'\(.*\)', line)[0]
        inputs = inputs[:-1] + "," + inputs[-1]

        # TODO: this is very hacky, we can do something fancier like parsing the stuff between the parentheses, injecting that into a dummy function, calling it, then getting the values of those variables.
        # this will be needed in case the inputs are not just variables but also function calls or other stuff
        # right now this just assumes the inputs are variables
        inputs: tuple = eval(inputs, copy.deepcopy(current_local_dict))

        # call the finetuned model with the inputs
        return self.finetuned_model(*inputs)

    def diff_write(self, prev, current):
        diff = dict(set(current.items()) - set(prev.items()))   
        diff = sorted(diff.items())
            
        for k, v in diff:
            if k in self.program_inputs and self.program_inputs[k] == v:
                continue

            self.trace_string += f"> {k} = {v}\n\n"


    def trace_dispatch(self, frame, event, arg):
        if not self.tracing_enabled:
            return
        
        # step into only the first call (program(**program_inputs) call)
        if self.first and event == "call":
            self.set_step()
            self.first = False
            return super().trace_dispatch(frame, event, arg)

        filename = frame.f_code.co_filename
        lineno = frame.f_lineno
        cur_line = linecache.getline(filename, lineno, frame.f_globals)
        current_local_dict = frame.f_locals
    
        ###### INFERENCE MODE ######
        # TODO: naive way to do it but it could be done very cleanly and robustly
        if self.finetuned_model and self.is_llm_call(cur_line):
            # TODO: now it assumes only one variable is being assigned per line. We should handle cases where a, b = x, y. Some eval things could be done here
            k = cur_line.split("=")[0].strip()
            if f"{k} =" in cur_line:
                # we are in inference mode
                # we want to call the finetuned model here and replace v with the generation of the finetuned model
                v = self.call_finetuned_model(cur_line, current_local_dict)
                self.generated_variables[k] = v

        for key, value in self.generated_variables.items():
            exec(f"{key} = {repr(value)}", frame.f_globals, frame.f_locals)        

        # only trace lines
        if event != "line":
            return 
        
        # if we have reached the end of the trace, stop tracing
        if cur_line.strip() == "set_trace(end=True)":
            return

        new_dict = {}
        for k, v in current_local_dict.items():
            try:
                new_dict[k] = str(v)
            except:
                pass

        current_local_dict = new_dict
        
        if self.prev_local_dict is None:
            # this is the first call after setting our debugger 
            self.first_write()
        else:

            next_lineno = frame.f_lineno + 1

            next_line = linecache.getline(filename, next_lineno).strip()

            self.diff_write(prev=self.prev_local_dict, current=current_local_dict)
        
        self.prev_local_dict = copy.deepcopy(current_local_dict)
    
        self.prev_line = cur_line

        # don't write the first line (program() call)
        if self.first and event == "line":
            return
        
        self.trace_string += cur_line.strip() + "\n"



def random_from_func():
    return random()

def get_traced_sample(program: callable, program_inputs: dict, finetuned_model: callable = None):
    execution_tracker = ExecutionTracker(program_inputs=program_inputs, finetuned_model=finetuned_model)

    # TODO: it makes more sense for this to be a method of execution_tracker and just call execution_tracker.set_trace() but it doesn't work for some reason
    def set_trace(end=False):
        if end:
            execution_tracker.tracing_enabled = False
            execution_tracker.trace_file = None
            return 

        execution_tracker.rcLines.append("next")
        execution_tracker.set_trace(sys._getframe().f_back)
        execution_tracker.tracing_enabled = True

    set_trace()  # Start the debugger here
    
    program(**program_inputs)

    set_trace(end=True)

    return execution_tracker.trace_string

def llm(prompt):
    return f"Hi its me llm {prompt}"

def finetuned_llm(prompt):
    return f"Hi its me FINETUNED {prompt}"


def traced_func(question):
    c = 3
    context = []
    for i in range(c):
        doc = llm(str(i))
        context.append(doc)
    return context

inp = {"question": "What?"}
### Example usage
print("Execution Mode (without finetuned model):")
print("=========================================")
sample = get_traced_sample(program=traced_func, program_inputs=inp)
print(sample)
print("=========================================")
print("Inference Mode (with finetuned model):")
print("=========================================")
sample = get_traced_sample(program=traced_func, program_inputs=inp, finetuned_model=finetuned_llm)
print(sample)